{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs’ potential is huge, because they can learn to **mimic any distribution of data**. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose.\n",
    "\n",
    "When this problem is expressed mathematically, the label is called y and the features are called x. The formulation p(y|x) is used to mean “the probability of y given x”, which in this case would translate to “the probability that an email is spam given the words it contains.”\n",
    "\n",
    "**So discriminative algorithms map features to labels**. They are concerned solely with that correlation. One way to think about **generative algorithms** is that they do the opposite. **Instead of predicting a label given certain features, they attempt to predict features given a certain label**.\n",
    "\n",
    "While discriminative models care about the relation between y and x, generative models care about “how you get x.” They allow you to **capture p(x|y), the probability of x given y, or the probability of features given a label or category.**\n",
    "\n",
    "Generative Adversarial Networks belong to the set of generative models. It means that they are able to produce / to generate (we’ll see how) new content. To illustrate this notion of “generative models.\n",
    "\n",
    "<img src=\"Images/q.png\" style=\"width: 450px;\">\n",
    "<img src=\"Images/GANs.png\" style=\"width: 450px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs, Autoencoders and VAEs\n",
    "It may be useful to compare generative adversarial networks to other neural networks, such as autoencoders and variational autoencoders.\n",
    "\n",
    "**Autoencoders encode input data as vectors.** They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation, much as you would with a restricted Boltzmann machine.\n",
    "\n",
    "**Variational autoencoders are generative algorithm** that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. **Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN.** However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred.\n",
    "\n",
    "You can bucket generative algorithms into one of three types:\n",
    "\n",
    "    Given a label, they predict the associated features (Naive Bayes)\n",
    "    Given a hidden representation, they predict the associated features (VAE, GAN)\n",
    "    Given some of the features, they predict the rest (inpainting, imputation)\n",
    "    \n",
    "<img src=\"Images/auto.jpg\" style=\"width: 450px;\">\n",
    "<img src=\"Images/qq.png\" style=\"width: 560px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They propose a new framework for estimating generative models via an adversarial process, in which they simultaneously train two models:  a generative model **G that captures the data distribution**, and a **discriminative model D that estimates the probability that a sample came from the training data rather than G.** The training procedure for G is to maximize the probability of D making a mistake.  **This framework corresponds to a minimax two-player game.** In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to everywhere. **In the case where G and D are definedby multilayer perceptrons, the entire system can be trained with backpropagation.** There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.\n",
    "\n",
    "<img src=\"Images/qqq.png\" style=\"width: 560px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "Our objective function is represented as a minimax function. The discriminator tries to maximize the objective function, therefore we can perform gradient ascent on the objective function. The generator tries to minimize the objective function, therefore we can perform gradient descent on the objective function. **By alternating between gradient ascent and descent, the network can be trained.**\n",
    "\n",
    "<img src=\"Images/r.png\" style=\"width: 560px;\">\n",
    "<img src=\"Images/rr.png\" style=\"width: 560px;\">\n",
    "<img src=\"Images/rrr.png\" style=\"width: 560px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are a new way to build generative modelsP(X).  GANs mayhave more flexibility and potential than VAEs.  They produce sharperand cleaner results than VAEs.  However, they are much harder totrain and have their own set of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "1. GANs are more unstable to train because you have to train two networks from a single backpropagation. Therefore choosing the right objectives can make a big difference.\n",
    "\n",
    "2. We cannot perform any inference queries with GANs\n",
    "3. **Hard to achieve Nash equilibrium.** Salimans et al. (2016) discussed the problem with GAN’s gradient-descent-based training procedure. Two models are trained simultaneously to find a Nash equilibrium to a two-player non-cooperative game. However, each model updates its cost independently with no respect to another player in the game. Updating the gradient of both models concurrently cannot guarantee a convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References   \n",
    "\n",
    "1.https://skymind.ai/wiki/generative-adversarial-network-gan    \n",
    "2.http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf  \n",
    "3.https://pdfs.semanticscholar.org/7dd9/e6f5a53f65afda3a5fd445ca4db132375103.pdf    \n",
    "4.https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf    \n",
    "5.https://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091   \n",
    "6.https://medium.com/deep-math-machine-learning-ai/ch-14-general-adversarial-networks-gans-with-math-1318faf46b43\n",
    "7.https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html  \n",
    "8.https://upcommons.upc.edu/bitstream/handle/2117/113330/memoria.pdf?sequence=1&isAllowed=y\n",
    "9.https://www.youtube.com/watch?v=uygj2tkm1Sg   \n",
    "10.http://www.rricard.me/machine/learning/generative/adversarial/networks/keras/tensorflow/2017/04/05/gans-part2.html\n",
    "11.\n",
    "12."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
